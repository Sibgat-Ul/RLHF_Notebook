{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49a28253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "# local_dir = snapshot_download(\"unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit\", local_dir=\"./md/unsloth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09f1c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# train_dataset = snapshot_download('PKU-Alignment/Align-Anything', repo_type=\"dataset\", allow_patterns=[\"text-image-to-text/train/train-00000-of-00015.parquet\"], local_dir='./ds2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeac6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/intisar/miniconda3/envs/llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 1 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.95s/it]\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# train_dataset = snapshot_download('trl-lib/rlaif-v', repo_type=\"dataset\", allow_patterns=[\"data/train-00004-of-00014.parquet\"], local_dir='./ds3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c151c71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "# One must patch the DPO Trainer first!\n",
    "from unsloth import PatchDPOTrainer\n",
    "\n",
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddba4975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.7: Fast Qwen2 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA RTX A4000. Num GPUs = 1. Max memory: 15.61 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096\n",
    "dtype = torch.bfloat16\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/media/intisar/dataset1/visual_categorization/Sigbath/md/unsloth\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea0da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @title Alignment Handbook utils\n",
    "# import os\n",
    "# import re\n",
    "# from typing import List, Literal, Optional\n",
    "\n",
    "# from datasets import DatasetDict, concatenate_datasets, load_dataset, load_from_disk\n",
    "# from datasets.builder import DatasetGenerationError\n",
    "\n",
    "\n",
    "# DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|im_start|>user\\n' + message['content'] + <|im_end|> + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|im_start|>system\\n' + message['content'] + <|im_end|> + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|im_start|>assistant\\n'  + message['content'] + <|im_end|> + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|im_start|>assistant' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "\n",
    "\n",
    "# def apply_chat_template(\n",
    "#     example,\n",
    "#     tokenizer,\n",
    "#     task: Literal[\"sft\", \"generation\", \"rm\", \"dpo\"] = \"sft\",\n",
    "#     assistant_prefix=\"<|im_start|>assistant\\n\",\n",
    "# ):\n",
    "#     def _strip_prefix(s, pattern):\n",
    "#         # Use re.escape to escape any special characters in the pattern\n",
    "#         return re.sub(f\"^{re.escape(pattern)}\", \"\", s)\n",
    "\n",
    "#     if task in [\"sft\", \"generation\"]:\n",
    "#         messages = example[\"messages\"]\n",
    "#         # We add an empty system message if there is none\n",
    "#         if messages[0][\"role\"] != \"system\":\n",
    "#             messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "#         example[\"text\"] = tokenizer.apply_chat_template(\n",
    "#             messages,\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=True if task == \"generation\" else False,\n",
    "#         )\n",
    "#     elif task == \"rm\":\n",
    "#         if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
    "#             chosen_messages = example[\"chosen\"]\n",
    "#             rejected_messages = example[\"rejected\"]\n",
    "#             # We add an empty system message if there is none\n",
    "#             if chosen_messages[0][\"role\"] != \"system\":\n",
    "#                 chosen_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "#             if rejected_messages[0][\"role\"] != \"system\":\n",
    "#                 rejected_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "#             example[\"text_chosen\"] = tokenizer.apply_chat_template(\n",
    "#                 chosen_messages, tokenize=False\n",
    "#             )\n",
    "#             example[\"text_rejected\"] = tokenizer.apply_chat_template(\n",
    "#                 rejected_messages, tokenize=False\n",
    "#             )\n",
    "#         else:\n",
    "#             raise ValueError(\n",
    "#                 f\"Could not format example as dialogue for `rm` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
    "#             )\n",
    "#     elif task == \"dpo\":\n",
    "#         if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
    "#             from PIL import Image\n",
    "#             import requests\n",
    "#             from io import BytesIO\n",
    "#             import base64\n",
    "\n",
    "#             img = BytesIO(example[\"image\"][\"bytes\"])\n",
    "#             image_data = base64.b64encode(img.getvalue()).decode('utf-8')\n",
    "\n",
    "#             prompt_message = [\n",
    "#                 {\n",
    "#                     \"role\": \"user\",\n",
    "#                     \"content\": [\n",
    "#                         {\n",
    "#                             \"type\": \"image\",\n",
    "#                             \"image\": f\"data:image;base64,{image_data}\" , \n",
    "#                         },\n",
    "#                         {\n",
    "#                             \"type\": \"text\",\n",
    "#                             \"text\": example[\"question\"],\n",
    "#                         },\n",
    "#                     ],\n",
    "#                 },\n",
    "\n",
    "#             ]\n",
    "\n",
    "#             chosen_messages = [{\n",
    "#                     \"role\": \"assistant\",\n",
    "#                     \"content\": [\n",
    "#                         {\n",
    "#                             \"type\": \"text\",\n",
    "#                             \"text\": example[\"chosen\"],  # Chosen response\n",
    "#                         },\n",
    "#                     ],\n",
    "#                 }]\n",
    "            \n",
    "#             rejected_messages = [{\n",
    "#                     \"role\": \"assistant\",\n",
    "#                     \"content\": [\n",
    "#                         {\n",
    "#                             \"type\": \"text\",\n",
    "#                             \"text\": example[\"rejected\"],  # Rejected response\n",
    "#                         },\n",
    "#                     ],\n",
    "#                 }]\n",
    "\n",
    "#             example[\"text_chosen\"] = tokenizer.apply_chat_template(\n",
    "#                 chosen_messages, tokenize=False\n",
    "#             )\n",
    "#             example[\"text_rejected\"] = tokenizer.apply_chat_template(\n",
    "#                 rejected_messages, tokenize=False\n",
    "#             )\n",
    "#             example[\"text_prompt\"] = tokenizer.apply_chat_template(\n",
    "#                 prompt_message, tokenize=False, add_generation_prompt=True\n",
    "#             )\n",
    "#             example[\"text_chosen\"] = _strip_prefix(\n",
    "#                 example[\"text_chosen\"], assistant_prefix\n",
    "#             )\n",
    "#             example[\"text_rejected\"] = _strip_prefix(\n",
    "#                 example[\"text_rejected\"], assistant_prefix\n",
    "#             )\n",
    "\n",
    "#             from qwen_vl_utils import process_vision_info\n",
    "#             image_inputs, video_inputs = process_vision_info(prompt_message)\n",
    "#             example[\"images\"] = image_inputs\n",
    "\n",
    "\n",
    "#         else:\n",
    "#             raise ValueError(\n",
    "#                 f\"Could not format example as dialogue for `dpo` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
    "#             )\n",
    "#     else:\n",
    "#         raise ValueError(\n",
    "#             f\"Task {task} not supported, please ensure that the provided task is one of {['sft', 'generation', 'rm', 'dpo']}\"\n",
    "#         )\n",
    "#     return example\n",
    "\n",
    "\n",
    "# def get_datasets(\n",
    "#     data_config: dict,\n",
    "#     name: Optional[str] = None,\n",
    "#     splits: List[str] = [\"train\", \"test\"],\n",
    "#     shuffle: bool = True,\n",
    "# ) -> DatasetDict:\n",
    "\n",
    "\n",
    "#     if type(data_config) is dict:\n",
    "#         dataset_mixer = data_config\n",
    "#     else:\n",
    "#         raise ValueError(f\"Data config {data_config} not recognized.\")\n",
    "\n",
    "#     raw_datasets = mix_datasets(dataset_mixer, name, splits=splits, shuffle=shuffle)\n",
    "#     return raw_datasets\n",
    "\n",
    "\n",
    "# def mix_datasets(\n",
    "#     dataset_mixer: dict, name:str, splits: Optional[List[str]] = None, shuffle=True\n",
    "# ) -> DatasetDict:\n",
    "\n",
    "#     raw_datasets = DatasetDict()\n",
    "#     raw_train_datasets = []\n",
    "#     raw_val_datasets = []\n",
    "#     fracs = []\n",
    "#     for ds, frac in dataset_mixer.items():\n",
    "#         fracs.append(frac)\n",
    "#         for split in splits:\n",
    "#             try:\n",
    "#                 dataset = load_dataset(ds, split=split, name=name)\n",
    "#             except DatasetGenerationError:\n",
    "#                 dataset = load_from_disk(os.path.join(ds, split))\n",
    "\n",
    "#             if \"train\" in split:\n",
    "#                 raw_train_datasets.append(dataset)\n",
    "#             elif \"test\" in split:\n",
    "#                 raw_val_datasets.append(dataset)\n",
    "#             else:\n",
    "#                 raise ValueError(\n",
    "#                     f\"Split type {split} not recognized as one of test or train.\"\n",
    "#                 )\n",
    "\n",
    "#     if any(frac < 0 for frac in fracs):\n",
    "#         raise ValueError(\"Dataset fractions cannot be negative.\")\n",
    "\n",
    "#     if len(raw_train_datasets) > 0:\n",
    "#         train_subsets = []\n",
    "#         for dataset, frac in zip(raw_train_datasets, fracs):\n",
    "#             train_subset = dataset.select(range(int(frac * len(dataset))))\n",
    "#             train_subsets.append(train_subset)\n",
    "#         if shuffle:\n",
    "#             raw_datasets[\"train\"] = concatenate_datasets(train_subsets).shuffle(seed=42)\n",
    "#         else:\n",
    "#             raw_datasets[\"train\"] = concatenate_datasets(train_subsets)\n",
    "#     # No subsampling for test datasets to enable fair comparison across models\n",
    "#     if len(raw_val_datasets) > 0:\n",
    "#         if shuffle:\n",
    "#             raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets).shuffle(\n",
    "#                 seed=42\n",
    "#             )\n",
    "#         else:\n",
    "#             raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets)\n",
    "\n",
    "#     if len(raw_datasets) == 0:\n",
    "#         raise ValueError(\n",
    "#             f\"Dataset {dataset_mixer} not recognized with split {split}. Check the dataset has been correctly formatted.\"\n",
    "#         )\n",
    "\n",
    "#     return raw_datasets\n",
    "\n",
    "# raw_datasets = get_datasets(\n",
    "#     {\"/media/intisar/dataset1/visual_categorization/Sigbath/ds\" : 0.005},\n",
    "#     splits=[\"train\"],\n",
    "# )\n",
    "\n",
    "# column_names = list(raw_datasets[\"train\"].features)\n",
    "\n",
    "# raw_datasets = raw_datasets.map(\n",
    "#     apply_chat_template,\n",
    "#     fn_kwargs = {\"tokenizer\": tokenizer, \"task\": \"dpo\"},\n",
    "#     num_proc = 12,\n",
    "#     remove_columns = column_names,\n",
    "#     desc = \"Formatting comparisons with prompt template\",\n",
    "# )\n",
    "\n",
    "# for split in [\"train\"]:\n",
    "#     raw_datasets[split] = raw_datasets[split].rename_columns(\n",
    "#         {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a411de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# def process_image(example):\n",
    "#     from qwen_vl_utils import process_vision_info\n",
    "#     image_inputs, video_inputs = process_vision_info(example[\"images\"])\n",
    "#     example[\"images\"] = image_inputs\n",
    "#     return example\n",
    "\n",
    "dataset = load_dataset(\"/media/intisar/dataset1/visual_categorization/Sigbath/ds3\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f21c9e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': [{'content': [{'text': Value(dtype='string', id=None),\n",
       "     'type': Value(dtype='string', id=None)}],\n",
       "   'role': Value(dtype='string', id=None)}],\n",
       " 'rejected': [{'content': [{'text': Value(dtype='string', id=None),\n",
       "     'type': Value(dtype='string', id=None)}],\n",
       "   'role': Value(dtype='string', id=None)}],\n",
       " 'prompt': [{'content': [{'text': Value(dtype='string', id=None),\n",
       "     'type': Value(dtype='string', id=None)}],\n",
       "   'role': Value(dtype='string', id=None)}],\n",
       " 'images': Sequence(feature=Image(mode=None, decode=True, id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "611c85a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 4,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e5a3781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import PatchDPOTrainer\n",
    "\n",
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86c5b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    ref_model = None,\n",
    "    args = DPOConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 3,\n",
    "        learning_rate = 5e-6,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.0,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    "    beta = 0.1,\n",
    "    train_dataset = dataset.select([i for i in range(0, len(dataset), 4)]),\n",
    "    # eval_dataset = raw_datasets[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    max_length = 8192,\n",
    "    max_prompt_length = 2048,\n",
    "    remove_unused_columns = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1daad4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'images', 'prompt_input_ids', 'pixel_values', 'chosen_input_ids', 'rejected_input_ids'],\n",
       "    num_rows: 1470\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_trainer.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66df53a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,470 | Num Epochs = 3 | Total steps = 552\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 18,576,384 of 3,773,199,360 (0.49% trained)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['prompt_input_ids', 'chosen_input_ids', 'rejected_input_ids']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdpo_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/accelerate/utils/memory.py:174\u001b[39m, in \u001b[36mfind_executable_batch_size.<locals>.decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo executable batch size found, reached zero.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:269\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/unsloth_zoo/loss_utils.py:294\u001b[39m, in \u001b[36m_unsloth_get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device, *args, **kwargs)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m         batch_samples += [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    296\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/accelerate/data_loader.py:567\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m.end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/data/data_collator.py:46\u001b[39m, in \u001b[36mDataCollatorMixin.__call__\u001b[39m\u001b[34m(self, features, return_tensors)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tf_call(features)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_tensors == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_tensors == \u001b[33m\"\u001b[39m\u001b[33mnp\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy_call(features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/data/data_collator.py:1013\u001b[39m, in \u001b[36mDataCollatorForLanguageModeling.torch_call\u001b[39m\u001b[34m(self, examples)\u001b[39m\n\u001b[32m   1010\u001b[39m     \u001b[38;5;28mself\u001b[39m.create_rng()\n\u001b[32m   1012\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples[\u001b[32m0\u001b[39m], Mapping):\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m     batch = \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1017\u001b[39m     batch = {\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: _torch_collate_batch(examples, \u001b[38;5;28mself\u001b[39m.tokenizer, pad_to_multiple_of=\u001b[38;5;28mself\u001b[39m.pad_to_multiple_of)\n\u001b[32m   1019\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/data/data_collator.py:67\u001b[39m, in \u001b[36mpad_without_fast_tokenizer_warning\u001b[39m\u001b[34m(tokenizer, *pad_args, **pad_kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     padded = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[32m     70\u001b[39m     tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = warning_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3303\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.pad\u001b[39m\u001b[34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[39m\n\u001b[32m   3301\u001b[39m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has been passed for padding\u001b[39;00m\n\u001b[32m   3302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[32m-> \u001b[39m\u001b[32m3303\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3304\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3305\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   3306\u001b[39m     )\n\u001b[32m   3308\u001b[39m required_input = encoded_inputs[\u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m]]\n\u001b[32m   3310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) == \u001b[32m0\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['prompt_input_ids', 'chosen_input_ids', 'rejected_input_ids']"
     ]
    }
   ],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd6f7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
